% Reference file generated in bibtex format

@InProceedings{exploit-semantics-lp-kg,
author="Chudasama, Yashrajsinh",
editor="Pesquita, Catia
and Skaf-Molli, Hala
and Efthymiou, Vasilis
and Kirrane, Sabrina
and Ngonga, Axel
and Collarana, Diego
and Cerqueira, Renato
and Alam, Mehwish
and Trojahn, Cassia
and Hertling, Sven",
title="Exploiting Semantics for Explaining Link Prediction Over Knowledge Graphs",
booktitle="The Semantic Web: ESWC 2023 Satellite Events",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="321--330",
abstract="The use of Symbolic and sub-symbolic AI techniques on Knowledge Graphs (KGs) has shown significant progress in several applications. However, many of these methods remain opaque, and the decision-making process behind them can be perplexing. This can result in a lack of trust and reliability in the overall framework. While various explainable frameworks have been proposed to address these issues, do not always provide a complete understanding and may raise privacy concerns as sensitive data may be revealed during the explanation process. In contrast, our proposed approach leverages the semantics of KGs and causal relationships to enhance explainability while still maintaining a high level of trust and reliability. By focusing on XAI for link prediction models and considering entailment regimes (e.g., rdfs:subPropertyOf), the approach can provide more comprehensive and accurate explanations. Moreover, the use of symbolic reasoning allows for more transparent and interpretable explanations. The preliminary results show that our approach is capable of exploiting the semantics of an entity in KG and enhancing the explanations. Henceforth, more work needs to be conducted, to fully comprehend all impacting factors and to identify the most relevant explanations of the machine learning models over KGs.",
isbn="978-3-031-43458-7"
}

@Article{sota-adv-att-def,
author={Zhai, Zhengli
and Li, Penghui
and Feng, Shu},
title={State of the art on adversarial attacks and defenses in graphs},
journal={Neural Computing and Applications},
year={2023},
month={Sep},
day={01},
volume={35},
number={26},
pages={18851-18872},
issn={1433-3058},
doi={10.1007/s00521-023-08839-9},
url={https://doi.org/10.1007/s00521-023-08839-9}
}

@misc{adv-att-kge-instance-attrib,
      title={Adversarial Attacks on Knowledge Graph Embeddings via Instance Attribution Methods}, 
      author={Peru Bhardwaj and John Kelleher and Luca Costabello and Declan O'Sullivan},
      year={2021},
      eprint={2111.03120},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{poison-att-kge,
      title={Data Poisoning Attack against Knowledge Graph Embedding}, 
      author={Hengtong Zhang and Tianhang Zheng and Jing Gao and Chenglin Miao and Lu Su and Yaliang Li and Kui Ren},
      year={2019},
      eprint={1904.12052},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{poison-kge-relation-infer-pattern,
   title={Poisoning Knowledge Graph Embeddings via Relation Inference Patterns},
   url={http://dx.doi.org/10.18653/v1/2021.acl-long.147},
   DOI={10.18653/v1/2021.acl-long.147},
   booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
   publisher={Association for Computational Linguistics},
   author={Bhardwaj, Peru and Kelleher, John and Costabello, Luca and O’Sullivan, Declan},
   year={2021} 
}

@InProceedings{eval-fw-poison-kge,
author="Zhu, Dong
and Lin, Yao
and Wang, Le
and Xie, Yushun
and Jiang, Jie
and Gu, Zhaoquan",
editor="Liu, Fei
and Duan, Nan
and Xu, Qingting
and Hong, Yu",
title="Evaluation Framework for Poisoning Attacks on Knowledge Graph Embeddings",
booktitle="Natural Language Processing and Chinese Computing",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="184--196",
abstract="In the area of knowledge graph embedding data poisoning, attackers are beginning to consider the importance of poisoning sample exposure risk while increasing the toxicity of the poisoning sample. On the other hand, we have found that some researchers incorrectly assess the effectiveness of poisoning attacks without considering the impact of the data-adding operation on model performance. Also, there is currently no definition of the Stealthiness of poisoning attacks. To address this issue, we provide an objective and unified framework for evaluating complex and diverse poisoning strategies. We design a controlled experiment on poisoning attacks to obtain objectively correct poisoning effects, and propose toxicity Dt to evaluate the poisoning performance of poisoning attacks and stealthiness Ds to evaluate the exposure risk of poisoning attacks. In designing the metrics, we fully considered the performance of the control model and the generalizability of the attacked model so that the data poisoning effect can be objectively and correctly evaluated. We compared 12 recently proposed KG attack methods on two different benchmark datasets to verify the objectivity and correctness of our evaluation criteria and to analyze the impact of the generalizability of the attacked model.",
isbn="978-3-031-44693-1"
}

